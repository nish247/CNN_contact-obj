{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa\n",
    "# !pip install tqdm\n",
    "# !pip install pycm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ライブラリのインポート \n",
    "import librosa\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display\n",
    "import pandas as pd\n",
    "import statistics as sta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(librosa.__version__)\n",
    "print(torch.__version__)\n",
    "\n",
    "#ライブラリ\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# t-SNE 特徴量空間上での可視化\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(0)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns; sns.set() # グラフ描画用\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pycm import ConfusionMatrix\n",
    "\n",
    "import csv\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データセットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 44100 #サンプリング周波数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 試行ごとに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #ファイルパスの指定\n",
    "# origin_data = \"data/origin_data/origindata_20240619\"\n",
    "\n",
    "# #学習用データの読み込み\n",
    "# audio_dir = origin_data\n",
    "# subFolders = [f for f in os.listdir(audio_dir) if os.path.isdir(os.path.join(audio_dir, f))]\n",
    "# subFolders = sorted(subFolders) #昇順に並び替え\n",
    "\n",
    "# #ファイル読み込み\n",
    "# # originDatasに音声データを格納していく\n",
    "# originDatas = []\n",
    "# for subFolder in subFolders:\n",
    "#     filePath = audio_dir+\"/\"+subFolder+\"/\"+subFolder+\".WAV\"\n",
    "#     originData, sr = librosa.load(filePath, sr = sr)\n",
    "#     originDatas.append(originData)\n",
    "#     print(subFolder)\n",
    "# #1データから30試行をトリミングを22パターン分行い，soundDataset_new_newに格納する\n",
    "# # soundDataset_new_newの構造：[パターン(10)][試行(50)][サンプリングデータ(66150)]\n",
    "\n",
    "# flag_amp = 0.1 #各試行の合図を検知する基準振幅\n",
    "# trimSkip = int(sr*0.4)\n",
    "# trimTime = int(sr*1.5)#1試行あたりのデータ長\n",
    "# dataNum = 50\n",
    "# trial = 0\n",
    "# soundDataset_new =([])\n",
    "\n",
    "# while trial <len(originDatas):\n",
    "#     index = 0\n",
    "#     trimDatas = ([]) #1データ(30試行)分のトリミングデータのリストを初期化\n",
    "#     originData = np.array(originDatas[trial])\n",
    "#     while index <len(originData):\n",
    "#         if originData[index] >= flag_amp:\n",
    "#             trimData = np.array(originData[index+trimSkip:index+trimTime+trimSkip]) #trim_dataにそのindexからindex+trimTimeのデータを格納する\n",
    "#             trimDatas = np.append(trimDatas ,trimData, axis=0)  #trimDatasに追加する\n",
    "#             index += trimTime\n",
    "#         else:\n",
    "#             index +=1\n",
    "#         if len(trimDatas) >=dataNum*trimTime:\n",
    "#             break\n",
    "#     soundDataset_new = np.append(soundDataset_new,np.array(trimDatas),axis=0)\n",
    "#     trial += 1\n",
    "\n",
    "# soundDataset_new = soundDataset_new.reshape(len(originDatas),dataNum,trimTime)\n",
    "\n",
    "# print(soundDataset_new.shape)\n",
    "\n",
    "# import pickle\n",
    "# f = open('soundDataset_20240619.pickle','wb')\n",
    "# pickle.dump(soundDataset_new,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ラベルを含んだデータセット化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('soundDataset_20240619.pickle','rb')\n",
    "soundDataset_new = pickle.load(f)\n",
    "\n",
    "patterns = soundDataset_new.shape[0] #物体の種類\n",
    "trials = soundDataset_new.shape[1] #試行数\n",
    "len_data = soundDataset_new.shape[2] #1試行の長さ,サンプル数\n",
    "num_data = patterns*trials #全試行数\n",
    "\n",
    "print(patterns,trials,len_data,num_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ラベルのリスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object層\n",
    "objLabel = [\n",
    "    \"y-shirt\", \n",
    "    \"jeans\", \n",
    "    \"sweatshirt\", \n",
    "\n",
    "    \"blanket\", \n",
    "    \"bedquilt\", \n",
    "    \"pillow\", \n",
    "       \n",
    "    \"mousepad\", \n",
    "    \"chair\", \n",
    "    \"sofa\", \n",
    "    \n",
    "    \"thick-Book\", \n",
    "    \"thin-Book\", \n",
    "    \"cardboard\", \n",
    "\n",
    "    \"metal-desk\", \n",
    "    \"laptop\", \n",
    "    \"alumi-rack\", \n",
    "    \n",
    "    \"wood-desk\", \n",
    "    \"wood-shelf\", \n",
    "    \"floor\", \n",
    "    \n",
    "    \"pla-Case\", \n",
    "    \"pla-Container\", \n",
    "    \"pla-Shelf\"\n",
    "]\n",
    "objNum= []\n",
    "\n",
    "#material層\n",
    "matLabel = [\n",
    "    \"clothing\",\n",
    "    \"bedding\", \n",
    "    \"memory-foam\", \n",
    "    \"paper\", \n",
    "    \"metal\", \n",
    "    \"wood\", \n",
    "    \"plastic\"\n",
    "]\n",
    "\n",
    "matNum = [0,1,2,3,4,5,6]\n",
    "\n",
    "#soft-hard層\n",
    "shLabel = [\n",
    "    \"soft\",\n",
    "    \"hard\",\n",
    "]\n",
    "\n",
    "shNum = [0,1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wavファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soundfile as sf\n",
    "\n",
    "# # wavファイルへの変換\n",
    "# def MakeTrimData(target_num,trial):\n",
    "#     # 既存のサンプル数66150の配列\n",
    "#     audio_data = soundDataset_new[target_num][trial]\n",
    "\n",
    "#     # WAVファイルに書き込む\n",
    "#     output_file = 'data/trim_data/trim-data_'+str(target_num)+'_'+str(trial)+'.wav'\n",
    "#     sample_rate = 44100  # サンプルレート（例: 44100 Hz）\n",
    "#     sf.write(output_file, audio_data, sample_rate)\n",
    "\n",
    "#     print(f\"WAVファイル '{output_file}' に書き込みました。\")\n",
    "\n",
    "# # wavファイルへの変換実行\n",
    "# for target_num in range(len(soundDataset_new)):\n",
    "#     for trial in range(len(soundDataset_new[0])):\n",
    "#         MakeTrimData(target_num,trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データフレーム化・CSVに書き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # num_feature次元のMFCCのデータフレームを作成\n",
    "\n",
    "# # ファイル名の生成と追加\n",
    "# filenames = []\n",
    "# for target_num in range(21):\n",
    "#     for trial in range(50):\n",
    "#         filenames.append(f'trim-data_{target_num}_{trial}.wav')\n",
    "\n",
    "# Dataset = pd.DataFrame()\n",
    "\n",
    "# Dataset['filename'] = filenames\n",
    "# Dataset['objectNum'] = [i // soundDataset_new.shape[1] for i in range(1050)]\n",
    "# Dataset['matNum'] = np.repeat(matNum, 150)[:1050]\n",
    "# Dataset['shNum'] = np.array([0]*450 + [1]*600)\n",
    "\n",
    "# # データセットに'objLabel'の列を追加\n",
    "# Dataset['object'] = np.repeat(objLabel, 50)[:1050]\n",
    "\n",
    "# # # データセットに'matLabel'の列を追加\n",
    "# Dataset['mat'] = np.repeat(matLabel, 150)[:1050]\n",
    "\n",
    "# # # データセットに'SHLabel'の列を追加\n",
    "# Dataset['sh'] = np.array(['soft']*450 + ['hard']*600)\n",
    "\n",
    "\n",
    "# # データフレームをCSVファイルに保存\n",
    "# csv_path = 'data/meta_data/dataset.csv'\n",
    "# Dataset.to_csv(csv_path, index=False)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "\n",
    "\n",
    "Dataset = pd.read_csv('data/meta_data/dataset.csv')\n",
    "Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCCのデータセット作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量の次元数\n",
    "num_feature = 40\n",
    "\n",
    "# 空のmfccSetを初期化\n",
    "mfccSet = np.empty((0, num_feature))\n",
    "\n",
    "# ループを使用してMFCCを抽出してmfccSetに追加\n",
    "for pattern in range(patterns):\n",
    "    for trial in range(trials):\n",
    "        # MFCCを計算\n",
    "        mfccs = librosa.feature.mfcc(y=soundDataset_new[pattern][trial], sr=sr)\n",
    "        # MFCCの各次元の平均を算出\n",
    "        mean = mfccs.mean(axis=1)\n",
    "        # MFCCの各次元の標準偏差を算出\n",
    "        std = np.std(mfccs, axis=1)\n",
    "        # mean, max_val, min_val, std をまとめた配列を作成\n",
    "        combined_stats = np.concatenate([mean,  std])\n",
    "        # mfccSetに追加\n",
    "        mfccSet = np.append(mfccSet, [combined_stats], axis=0)\n",
    "\n",
    "# 形状を確認\n",
    "print(mfccSet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイル名の生成と追加\n",
    "\n",
    "Dataset_mfcc = pd.DataFrame(data=mfccSet)\n",
    "\n",
    "Dataset_mfcc['objNum'] = [i // soundDataset_new.shape[1] for i in range(1050)]\n",
    "Dataset_mfcc['matNum'] = np.repeat(matNum, 150)[:1050]\n",
    "Dataset_mfcc['shNum'] = np.array([0]*450 + [1]*600)\n",
    "\n",
    "# データセットに'objLabel'の列を追加\n",
    "Dataset_mfcc['obj'] = np.repeat(objLabel, 50)[:1050]\n",
    "\n",
    "# # データセットに'matLabel'の列を追加\n",
    "Dataset_mfcc['mat'] = np.repeat(matLabel, 150)[:1050]\n",
    "\n",
    "# # データセットに'SHLabel'の列を追加\n",
    "Dataset_mfcc['sh'] = np.array(['soft']*450 + ['hard']*600)\n",
    "\n",
    "\n",
    "# データフレームをCSVファイルに保存\n",
    "# csv_path = 'data/meta_data/dataset_vib_mfcc.csv'\n",
    "# Dataset_mfcc.to_csv(csv_path, index=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "pd.set_option('display.max_rows',100)\n",
    "\n",
    "Dataset_mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メルスペクトログラムのデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パターンごとにスペクロログラムの保存\n",
    "Dataset_melspec = []\n",
    "trial_num = 50\n",
    "for patern in range (len(soundDataset_new)):\n",
    "    for trial in range(trial_num):\n",
    "        trimData = soundDataset_new[patern,trial]\n",
    "        spectrogram = librosa.feature.melspectrogram(y=trimData, sr=sr)# スペクトログラムを計算\n",
    "        spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)    \n",
    "        Dataset_melspec.append(spectrogram_db)\n",
    "\n",
    "Dataset_melspec = np.array(Dataset_melspec)\n",
    "Dataset_melspec = np.expand_dims(Dataset_melspec,1)\n",
    "\n",
    "print(Dataset_melspec.shape)\n",
    "print(type(Dataset_melspec))\n",
    "\n",
    "# ラベルのデータセット\n",
    "Dataset_label = Dataset.iloc[:,1:4]\n",
    "Dataset_label_np = Dataset_label.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm提案手法評価用\n",
    "# split (stratify by the first column of labels as an example)\n",
    "train_data_mfcc, eval_data_mfcc, train_label_mfcc, eval_label_mfcc = train_test_split(\n",
    "    Dataset_mfcc.iloc[:,1:num_feature], Dataset_mfcc.iloc[:,40:43], \n",
    "    stratify=Dataset_label_np[:, 0], shuffle=True, random_state=0, train_size=0.6\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンソルに変換\n",
    "Dataset_melspec_tensor = torch.tensor(Dataset_melspec, dtype=torch.float32)\n",
    "Dataset_label_tensor = torch.tensor(Dataset_label_np, dtype=torch.int64)\n",
    "\n",
    "# split (stratify by the first column of labels as an example)\n",
    "train_feature, eval_feature, train_labels, eval_labels = train_test_split(\n",
    "    Dataset_melspec_tensor, Dataset_label_tensor, \n",
    "    stratify=Dataset_label_np[:, 0], shuffle=True, random_state=0, train_size=0.7\n",
    ")\n",
    "\n",
    "val_feature, test_feature, val_labels, test_labels = train_test_split(\n",
    "    eval_feature, eval_labels, \n",
    "    stratify=eval_labels[:, 0], shuffle=True, random_state=0, train_size=0.5\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データにない物体の評価用データセットの作成\n",
    "def Make_other_dataset(num):\n",
    "\n",
    "    start = num*trials\n",
    "    end = (num+1)*trials\n",
    "\n",
    "    #テストデータの抽出\n",
    "    test_feature = Dataset_melspec_tensor[start:end,:,:,:]\n",
    "    test_labels = Dataset_label_tensor[start:end,:]\n",
    "\n",
    "    #テストデータ以外でデータセットを作成\n",
    "    other_feature = torch.cat((Dataset_melspec_tensor[:start,:,:,:],Dataset_melspec_tensor[end:,:,:,:]),dim=0)\n",
    "    other_labels = torch.cat((Dataset_label_tensor[:start,:],Dataset_label_tensor[end:,:]),dim=0)\n",
    "\n",
    "    #trainとvalに分割\n",
    "    train_feature, val_feature, train_labels, val_labels = train_test_split(\n",
    "        other_feature, other_labels, \n",
    "    stratify=other_labels[:, 0], shuffle=True, random_state=0, train_size=0.8\n",
    "    )   \n",
    "    return train_feature,train_labels, val_feature, val_labels,test_feature,test_labels\n",
    "\n",
    "# 使用\n",
    "# train_feature,train_labels, val_feature, val_labels,test_feature,test_labels = Make_other_dataset(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNEで特徴量空間の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30.0,\n",
    "    random_state=(0)\n",
    ")\n",
    "X_tsne = tsne.fit_transform(Dataset_mfcc.iloc[:,1:num_feature])\n",
    "\n",
    "# プロット\n",
    "title = \"contacting-object_t-SNE_map\"\n",
    "plt.title(title)\n",
    "\n",
    "scatter=plt.scatter(\n",
    "            X_tsne[:, 0], \n",
    "            X_tsne[:, 1], \n",
    "            c=Dataset_mfcc['objNum'], \n",
    "            cmap='Spectral',\n",
    "            edgecolor='black',\n",
    "            linewidths=0.05,\n",
    "            s=10 \n",
    "        )  \n",
    "# 凡例を追加\n",
    "plt.legend(handles=scatter.legend_elements(num=21)[0],labels=objLabel,bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"data/output_data/\"+title+\".pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMでの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eval_svm(subject,trainDataset,labelDataset,target,save):\n",
    "    #機械学習アルゴリズムの選択\n",
    "    model = svm.SVC(kernel=\"linear\",C =0.01,random_state = 0) \n",
    "\n",
    "    #10分割交差検証\n",
    "    folds = 10 #分割数\n",
    "    stratifiedkfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0) #10分割交差検証のパラメータ定義\n",
    "\n",
    "    pred = cross_val_predict(model, trainDataset,labelDataset, cv=stratifiedkfold)\n",
    "    \n",
    "    #classification_reportの作成\n",
    "    report = classification_report(labelDataset, pred,digits=3,target_names=target)\n",
    "    print(report)\n",
    "    \n",
    "    #混同行列の作成\n",
    "    cm = confusion_matrix(y_true=labelDataset,y_pred=pred)\n",
    "    p = sns.heatmap(cm, square=True, cbar=True, annot=True,cmap='Blues',fmt=\"d\",\n",
    "                    label=target,\n",
    "                    xticklabels=target,\n",
    "                    yticklabels=target)\n",
    "    title = \"confusion_matrix_\"+subject\n",
    "    plt.title(title)\n",
    "    plt.xlabel('predict')\n",
    "    plt.ylabel('actual')\n",
    "\n",
    "    #保存設定\n",
    "    if save == True:\n",
    "        plt.savefig('data/output_data/'+title+'.pdf', bbox_inches='tight')\n",
    "        with open('data/output_data/eval_report_'+subject+'.pdf','w') as f:\n",
    "            print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eval_svm_LOOO(subject,trainDataset,labelDataset,target,save):\n",
    "    #機械学習アルゴリズムの選択\n",
    "    model = svm.SVC(kernel=\"linear\",C =0.01,random_state = 0) \n",
    "\n",
    "    #10分割交差検証\n",
    "    folds = 21 #分割数\n",
    "    stratifiedkfold = KFold(n_splits=folds, shuffle=False) #10分割交差検証のパラメータ定義\n",
    "\n",
    "    pred = cross_val_predict(model, trainDataset,labelDataset, cv=stratifiedkfold)\n",
    "    \n",
    "    #classification_reportの作成\n",
    "    report = classification_report(labelDataset, pred,digits=3,target_names=target)\n",
    "    print(report)\n",
    "    \n",
    "    #混同行列の作成\n",
    "    cm = confusion_matrix(y_true=labelDataset,y_pred=pred)\n",
    "    p = sns.heatmap(cm, square=True, cbar=True, annot=True,cmap='Blues',fmt=\"d\",\n",
    "                    label=target,\n",
    "                    xticklabels=target,\n",
    "                    yticklabels=target)\n",
    "    title = \"confusion_matrix_\"+subject\n",
    "    plt.title(title)\n",
    "    plt.xlabel('predict')\n",
    "    plt.ylabel('actual')\n",
    "\n",
    "    #保存設定\n",
    "    if save == True:\n",
    "        plt.savefig('data/output_data/'+title+'.pdf', bbox_inches='tight')\n",
    "        with open('data/output_data/eval_report_'+subject+'.pdf','w') as f:\n",
    "            print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### object層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eval_svm(\n",
    "    \"object\",\n",
    "    Dataset_mfcc.iloc[:,1:num_feature],\n",
    "    Dataset_mfcc.obj,\n",
    "    objLabel,\n",
    "    False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eval_svm_LOOO(\n",
    "    \"object\",\n",
    "    Dataset_mfcc.iloc[:,1:num_feature],\n",
    "    Dataset_mfcc.obj,\n",
    "    objLabel,\n",
    "    False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( Dataset_mfcc.obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### material層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eval_svm(\n",
    "    \"material\",\n",
    "    Dataset_mfcc.iloc[:,1:num_feature],\n",
    "    Dataset_mfcc.mat,\n",
    "    matLabel,\n",
    "    False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eval_svm_LOOO(\n",
    "    \"material\",\n",
    "    Dataset_mfcc.iloc[:,1:num_feature],\n",
    "    Dataset_mfcc.mat,\n",
    "    matLabel,\n",
    "    False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft-hard層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eval_svm(\n",
    "    \"soft-hard\",\n",
    "    Dataset_mfcc.iloc[:,1:num_feature],\n",
    "    Dataset_mfcc.sh,\n",
    "    shLabel,\n",
    "    False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eval_svm_LOOO(\n",
    "    \"soft-hard\",\n",
    "    Dataset_mfcc.iloc[:,1:num_feature],\n",
    "    Dataset_mfcc.sh,\n",
    "    shLabel,\n",
    "    False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提案手法適用時の評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm提案手法評価用\n",
    "# split (stratify by the first column of labels as an example)\n",
    "train_data_mfcc = []\n",
    "train_label_mfcc = []\n",
    "eval_data_mfcc = []\n",
    "eval_label_mfcc = []\n",
    "\n",
    "train_data_mfcc, eval_data_mfcc, train_label_mfcc, eval_label_mfcc = train_test_split(\n",
    "    Dataset_mfcc.iloc[:,1:num_feature], Dataset_mfcc.iloc[:,40:43], \n",
    "    stratify=Dataset_label_np[:, 0], shuffle=True, random_state=0, train_size=0.6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_label_mfcc.objNum)\n",
    "model = svm.SVC(kernel=\"linear\",C =0.01,random_state = 0,probability=True) \n",
    "model.fit(train_data_mfcc,train_label_mfcc.objNum)\n",
    "\n",
    "#推定\n",
    "pred = model.predict(eval_data_mfcc)\n",
    "\n",
    "#推定確信度の算出\n",
    "Dec = model.predict_proba(eval_data_mfcc)\n",
    "print(Dec.shape)\n",
    "\n",
    "df = pd.DataFrame(Dec,columns = objLabel)\n",
    "df[\"decision\"] = np.max(Dec, axis=1)\n",
    "df[\"prediction\"] = pred\n",
    "df['Actual'] = np.array(eval_label_mfcc.objNum)\n",
    "\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "\n",
    "len_df = len(df.index)\n",
    "print(len_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均を算出し、物体レベルの閾値を決定する\n",
    "\n",
    "# predictionとActualが同じ場合の行を抽出\n",
    "filtered_rows = df[df['prediction'] == df['Actual']]\n",
    "\n",
    "# decision列の平均を計算\n",
    "average_decision_object = filtered_rows['decision'].mean()\n",
    "\n",
    "\n",
    "print(\"平均 decision:\", average_decision_object)\n",
    "# 算出した推定確信度で閾値を超えていたら、超えているもので推定結果をだす、超えていないものでDFを作成\n",
    "\n",
    "# 閾値以上を抽出しDFを作成\n",
    "Up_object = df[df['decision'] >= average_decision_object]\n",
    "\n",
    "\n",
    "print(classification_report(Up_object['Actual'], Up_object['prediction'],digits=3))\n",
    "\n",
    "print(\"採用数:\", len(Up_object.index))\n",
    "print(\"採用率:\", len(Up_object.index)/len_df)\n",
    "# 閾値未満を抽出しDFを作成\n",
    "Down_object = df[df['decision'] < average_decision_object]\n",
    "\n",
    "print(\"不採用数:\",Down_object.shape)\n",
    "# Down_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_label_mfcc.objNum)\n",
    "model = svm.SVC(kernel=\"linear\",C =0.01,random_state = 0,probability=True) \n",
    "model.fit(train_data_mfcc,train_label_mfcc.matNum)\n",
    "\n",
    "#推定\n",
    "pred = model.predict(eval_data_mfcc)\n",
    "\n",
    "#推定確信度の算出\n",
    "Dec = model.predict_proba(eval_data_mfcc)\n",
    "print(Dec.shape)\n",
    "\n",
    "df = pd.DataFrame(Dec,columns = matLabel)\n",
    "df[\"decision\"] = np.max(Dec, axis=1)\n",
    "df[\"prediction\"] = pred\n",
    "df['Actual'] = np.array(eval_label_mfcc.matNum)\n",
    "\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "\n",
    "len_df = len(df.index)\n",
    "print(len_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均を算出し、物体レベルの閾値を決定する\n",
    "\n",
    "# predictionとActualが同じ場合の行を抽出\n",
    "filtered_rows = df[df['prediction'] == df['Actual']]\n",
    "\n",
    "# decision列の平均を計算\n",
    "average_decision_material = filtered_rows['decision'].mean()\n",
    "\n",
    "\n",
    "print(\"平均 decision:\", average_decision_material)\n",
    "\n",
    "# 算出した推定確信度で閾値を超えていたら、超えているもので推定結果をだす、超えていないものでDFを作成\n",
    "# 閾値以上を抽出しDFを作成\n",
    "df = df.iloc[Down_object.index]\n",
    "Up_material = df[df['decision'] >= average_decision_material]\n",
    "\n",
    "\n",
    "print(classification_report(Up_material['Actual'], Up_material['prediction'],digits=3))\n",
    "\n",
    "print(\"採用数:\", len(Up_material.index))\n",
    "print(\"採用率:\", len(Up_material.index)/len_df)\n",
    "\n",
    "# 閾値未満を抽出しDFを作成\n",
    "Down_material = df[df['decision'] < average_decision_material]\n",
    "\n",
    "print(\"不採用:\",Down_material.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_label_mfcc.objNum)\n",
    "model = svm.SVC(kernel=\"linear\",C =0.01,random_state = 0,probability=True) \n",
    "model.fit(train_data_mfcc,train_label_mfcc.shNum)\n",
    "\n",
    "#推定\n",
    "pred = model.predict(eval_data_mfcc)\n",
    "\n",
    "#推定確信度の算出\n",
    "Dec = model.predict_proba(eval_data_mfcc)\n",
    "print(Dec.shape)\n",
    "\n",
    "df = pd.DataFrame(Dec,columns = shLabel)\n",
    "df[\"decision\"] = np.max(Dec, axis=1)\n",
    "df[\"prediction\"] = pred\n",
    "df['Actual'] = np.array(eval_label_mfcc.shNum)\n",
    "\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "\n",
    "len_df = len(df.index)\n",
    "print(len_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均を算出し、物体レベルの閾値を決定する\n",
    "\n",
    "# predictionとActualが同じ場合の行を抽出\n",
    "filtered_rows = df[df['prediction'] == df['Actual']]\n",
    "\n",
    "# decision列の平均を計算\n",
    "average_decision_softhard = filtered_rows['decision'].mean()\n",
    "\n",
    "print(\"平均 decision:\", average_decision_softhard)\n",
    "\n",
    "# 算出した推定確信度で閾値を超えていたら、超えているもので推定結果をだす、超えていないものでDFを作成\n",
    "# 閾値以上を抽出しDFを作成\n",
    "df = df.iloc[Down_material.index]\n",
    "Up_softhard = df[df['decision'] >= average_decision_softhard]\n",
    "\n",
    "\n",
    "print(classification_report(Up_softhard['Actual'], Up_softhard['prediction'],digits=3))\n",
    "\n",
    "print(\"採用数:\", len(Up_softhard.index))\n",
    "print(\"採用率:\", len(Up_softhard.index)/len_df)\n",
    "\n",
    "# 閾値未満を抽出しDFを作成\n",
    "Down_softhard = df[df['decision'] < average_decision_softhard]\n",
    "\n",
    "print(\"不採用:\",Down_softhard.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNでの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature,train_labels, val_feature, val_labels,test_feature,test_labels = Make_other_dataset(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの保存関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveModel(model,modelname):\n",
    "    # 日本時間のタイムゾーンを取得\n",
    "    jst = pytz.timezone('Asia/Tokyo')\n",
    "    # 現在の日本時間を取得\n",
    "    now = datetime.now(jst)\n",
    "    # 時刻を指定された形式の文字列に変換\n",
    "    time = \"_\"+now.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    model_path = 'data/models_saved/'+modelname+time+'.pth'\n",
    "    torch.save(model, model_path)\n",
    "\n",
    "# #読み込み用\n",
    "# model_path = 'data/models_saved/shModel_1_202406281304.pth'\n",
    "# xModel = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft-hard 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ラベルの指定\n",
    "# t_df = Dataset.shNum\n",
    "\n",
    "# # テンソル形式に変換\n",
    "# df_tenosor = torch.tensor(Dataset_melspec, dtype=torch.float32)\n",
    "# df_tenosor = torch.unsqueeze(df_tenosor, 1)\n",
    "# label_tensor = torch.tensor(t_df, dtype=torch.int64)\n",
    "# print(df_tenosor.size())\n",
    "\n",
    "# # x と t を組み合わせて TensorDataset を作成\n",
    "# dataset = torch.utils.data.TensorDataset(df_tenosor, label_tensor)\n",
    "\n",
    "# # 各データセットのサンプル数を決定\n",
    "# # train : val: test = 50%　: 25% : 25%\n",
    "# n_train = int(len(dataset) * 0.5)\n",
    "# n_val = int(len(dataset) * 0.25)\n",
    "# n_test = len(dataset) - n_train - n_val\n",
    "     \n",
    "# # それぞれのサンプル数を確認\n",
    "# print(n_train, n_val, n_test)\n",
    "\n",
    "# # ランダムに分割を行うため、シードを固定して再現性を確保\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# # データセットの分割\n",
    "# train, val, test = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "# # バッチサイズ\n",
    "# batch_size = 25\n",
    "\n",
    "      \n",
    "# # shuffle はデフォルトで False のため、学習データのみ True に指定\n",
    "# train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val, batch_size)\n",
    "# test_loader = torch.utils.data.DataLoader(test, batch_size)\n",
    "\n",
    "# print(train_loader)\n",
    "\n",
    "# for batch in train_loader:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorDatasetの作成\n",
    "label_num = 2\n",
    "train = torch.utils.data.TensorDataset(train_feature, train_labels[:,label_num])\n",
    "val = torch.utils.data.TensorDataset(val_feature, val_labels[:,label_num])\n",
    "test = torch.utils.data.TensorDataset(test_feature, test_labels[:,label_num])\n",
    "\n",
    "# データローダに読み込む\n",
    "\n",
    "# バッチサイズ\n",
    "batch_size = 25\n",
    "\n",
    "# shuffle はデフォルトで False のため、学習データのみ True に指定\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelのインポート\n",
    "from models.dymn.model import get_model as get_dymn\n",
    "shModel = get_dymn(pretrained_name=\"dymn10_as\")\n",
    "\n",
    "#パラメータの更新を許可\n",
    "for param in shModel.parameters():\n",
    "    param.requires_gred = False\n",
    "\n",
    "\n",
    "# print(model.classifier)\n",
    "\n",
    "# 出力層の最後だけ変更\n",
    "# model.classifier[5] = torch.nn.Linear(1280,2)\n",
    "shModel.classifier= nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(output_size=1),\n",
    "    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    nn.Linear(in_features=960, out_features=1280, bias=True),\n",
    "    nn.Hardswish(),\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=527, bias=True),\n",
    "    nn.BatchNorm1d(527),\n",
    "    nn.Linear(in_features=527, out_features=2, bias=True),  # 新しい層\n",
    ")\n",
    "print(shModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install torchsummary\n",
    "# import torchsummary\n",
    "\n",
    "# summary = torchsummary.summary(model, (1, 128, 130))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# エポックの数\n",
    "epoch = 0\n",
    "max_epoch = 10\n",
    "     \n",
    "# モデルの初期化\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# GPU の設定状況に基づいたデバイスの選択\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# モデルのインスタンス化とデバイスへの転送\n",
    "net1 = shModel.cuda()\n",
    "\n",
    "criterion = F.cross_entropy\n",
    "  \n",
    "# 最適化手法の選択\n",
    "optimizer = torch.optim.SGD(net1.parameters(), lr=0.1)\n",
    "# エポックごとの訓練データの損失と検証データの損失を保存するリスト\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_epoch_loss = 1\n",
    "val_epoch_loss = 1\n",
    "# 学習ループ\n",
    "while (train_epoch_loss > 0.1 or val_epoch_loss  > 0.4) and epoch<max_epoch:\n",
    "    # 訓練データでの学習\n",
    "    net1.train()\n",
    "    for batch in train_loader:\n",
    "        x, t = batch\n",
    "        x = x.to(device)        \n",
    "        t = t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y = net1(x)\n",
    "    \n",
    "        loss = criterion(y[0], t)\n",
    "        train_epoch_loss += loss.item()  # エポック全体の訓練データの損失に加算\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 訓練データでのエポックごとの平均損失を計算し保存\n",
    "    train_epoch_loss /= len(train_loader)\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    \n",
    "    # 検証データでの評価\n",
    "    net1.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, t = batch\n",
    "  \n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = net1(x)\n",
    "            loss = criterion(y[0], t)\n",
    "            val_epoch_loss += loss.item()  # エポック全体の検証データの損失に加算\n",
    "       \n",
    "    # 検証データでのエポックごとの平均損失を計算し保存\n",
    "    val_epoch_loss /= len(val_loader)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    \n",
    "    # エポックごとに損失を表示\n",
    "    print(f'Epoch [{epoch+1}/{max_epoch}], Train Loss: {train_epoch_loss:.4f}, Validation Loss: {val_epoch_loss:.4f}')\n",
    "    epoch+=1\n",
    "    \n",
    "shModel_trained = net1\n",
    "SaveModel(shModel_trained,\"shModel_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShowTrainLoss(train_losses,val_losses,save):\n",
    "    # 日本時間のタイムゾーンを取得\n",
    "    jst = pytz.timezone('Asia/Tokyo')\n",
    "\n",
    "    # 現在の日本時間を取得\n",
    "    now = datetime.now(jst)\n",
    "\n",
    "    # 時刻を指定された形式の文字列に変換\n",
    "    time = \"_\"+now.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    # 学習曲線の可視化\n",
    "\n",
    "    title = 'Training and Validation Losses Soft-hard'\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Validation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "    if save == True:\n",
    "        plt.savefig(\"data/\"+title+time+\".pdf\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "ShowTrainLoss(train_losses,val_losses,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'data/models_saved/shModel_2_202406281354.pth'\n",
    "# net1 = torch.load(model_path)\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# 正解率の計算\n",
    "def calc_acc(data_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        accs = [] # 各バッチごとの結果格納用\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            x, t = batch\n",
    "            \n",
    "            # x = torch.unsqueeze(x,1)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = net1(x)\n",
    "            \n",
    "            y_label = torch.argmax(y[0], dim=1)\n",
    "            acc = torch.sum(y_label == t) * 1.0 / len(t)\n",
    "            accs.append(acc)\n",
    "            \n",
    "    # 全体の平均を算出\n",
    "    avg_acc = torch.tensor(accs).mean()\n",
    "    print('Accuracy: {:.1f}%'.format(avg_acc * 100))\n",
    "    \n",
    "    return avg_acc\n",
    "      \n",
    "# 検証データで確認\n",
    "calc_acc(val_loader)\n",
    "# テストデータで確認\n",
    "calc_acc(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### material 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorDatasetの作成\n",
    "label_num = 1\n",
    "train = torch.utils.data.TensorDataset(train_feature, train_labels[:,label_num])\n",
    "val = torch.utils.data.TensorDataset(val_feature, val_labels[:,label_num])\n",
    "test = torch.utils.data.TensorDataset(test_feature, test_labels[:,label_num])\n",
    "\n",
    "# データローダに読み込む\n",
    "\n",
    "# バッチサイズ\n",
    "batch_size = 50\n",
    "\n",
    "# shuffle はデフォルトで False のため、学習データのみ True に指定\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matModel = shModel_trained\n",
    "\n",
    "#パラメータの更新を許可\n",
    "for param in matModel.parameters():\n",
    "    param.requires_gred = False\n",
    "\n",
    "\n",
    "# print(model.classifier)\n",
    "\n",
    "# 出力層の最後だけ変更\n",
    "# model.classifier[5] = torch.nn.Linear(1280,2)\n",
    "matModel.classifier= nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(output_size=1),\n",
    "    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    nn.Linear(in_features=960, out_features=1280, bias=True),\n",
    "    nn.Hardswish(),\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=527, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=527, out_features=7, bias=True),  # 新しい層\n",
    ")\n",
    "print(matModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# エポックの数\n",
    "epoch = 0\n",
    "max_epoch = 200\n",
    "# モデルの初期化\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# GPU の設定状況に基づいたデバイスの選択\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# モデルのインスタンス化とデバイスへの転送\n",
    "net2 = matModel.cuda()\n",
    "\n",
    "criterion = F.cross_entropy\n",
    "  \n",
    "# 最適化手法の選択\n",
    "optimizer = torch.optim.SGD(net2.parameters(), lr=0.1)\n",
    "# エポックごとの訓練データの損失と検証データの損失を保存するリスト\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_epoch_loss = 1.0\n",
    "val_epoch_loss = 1.0\n",
    "# 学習ループ\n",
    "while (train_epoch_loss > 0.1 or val_epoch_loss  > 0.5) and epoch<max_epoch:\n",
    "    train_epoch_loss = 0.0\n",
    "    val_epoch_loss = 0.0\n",
    "    \n",
    "    # 訓練データでの学習\n",
    "    net2.train()\n",
    "    for batch in train_loader:\n",
    "        x, t = batch\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y = net2(x)\n",
    "        loss = criterion(y[0], t)\n",
    "        train_epoch_loss += loss.item()  # エポック全体の訓練データの損失に加算\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 訓練データでのエポックごとの平均損失を計算し保存\n",
    "    train_epoch_loss /= len(train_loader)\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    \n",
    "    # 検証データでの評価\n",
    "    net2.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, t = batch\n",
    "  \n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = net2(x)\n",
    "            loss = criterion(y[0], t)\n",
    "            val_epoch_loss += loss.item()  # エポック全体の検証データの損失に加算\n",
    "       \n",
    "    # 検証データでのエポックごとの平均損失を計算し保存\n",
    "    val_epoch_loss /= len(val_loader)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    \n",
    "    # エポックごとに損失を表示\n",
    "    print(f'Epoch [{epoch+1}/{max_epoch}], Train Loss: {train_epoch_loss:.4f}, Validation Loss: {val_epoch_loss:.4f}')\n",
    "    epoch+=1\n",
    "\n",
    "matModel_trained = net2\n",
    "SaveModel(matModel_trained,\"matModel_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "def ShowTrainLoss(train_losses,val_losses,save):\n",
    "    # 日本時間のタイムゾーンを取得\n",
    "    jst = pytz.timezone('Asia/Tokyo')\n",
    "\n",
    "    # 現在の日本時間を取得\n",
    "    now = datetime.now(jst)\n",
    "\n",
    "    # 時刻を指定された形式の文字列に変換\n",
    "    time = \"_\"+now.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    # 学習曲線の可視化\n",
    "\n",
    "    title = 'Training and Validation Losses Material'\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Validation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "    if save == True:\n",
    "        plt.savefig(\"data/\"+title+time+\".pdf\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "ShowTrainLoss(train_losses,val_losses,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dim=1 で行ごとの最大値に対する要素番号を取得（dim=0 は列ごと）\n",
    "y_label = torch.argmax(y[0], dim=1)\n",
    "# 予測値から最大となるクラスの番号を取り出した結果\n",
    "y_label\n",
    "# 目的変数\n",
    "t\n",
    "# 値が一致しているか確認\n",
    "y_label == t\n",
    "# int => float \n",
    "torch.sum(y_label == t) * 1.0\n",
    "# 正解率\n",
    "acc = torch.sum(y_label == t) * 1.0 / len(t)\n",
    "acc\n",
    "\n",
    "\n",
    "# 正解率の計算\n",
    "def calc_acc(data_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        accs = [] # 各バッチごとの結果格納用\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            x, t = batch\n",
    "            \n",
    "            # x = torch.unsqueeze(x,1)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = net2(x)\n",
    "            \n",
    "            y_label = torch.argmax(y[0], dim=1)\n",
    "            acc = torch.sum(y_label == t) * 1.0 / len(t)\n",
    "            accs.append(acc)\n",
    "            \n",
    "    # 全体の平均を算出\n",
    "    avg_acc = torch.tensor(accs).mean()\n",
    "    print('Accuracy: {:.1f}%'.format(avg_acc * 100))\n",
    "    \n",
    "    return avg_acc\n",
    "      \n",
    "# 検証データで確認\n",
    "calc_acc(val_loader)\n",
    "# テストデータで確認\n",
    "calc_acc(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### object 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorDatasetの作成\n",
    "label_num = 0\n",
    "train = torch.utils.data.TensorDataset(train_feature, train_labels[:,label_num])\n",
    "val = torch.utils.data.TensorDataset(val_feature, val_labels[:,label_num])\n",
    "test = torch.utils.data.TensorDataset(test_feature, test_labels[:,label_num])\n",
    "\n",
    "# データローダに読み込む\n",
    "\n",
    "# バッチサイズ\n",
    "batch_size = 25\n",
    "\n",
    "# shuffle はデフォルトで False のため、学習データのみ True に指定\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objModel = matModel_trained\n",
    "\n",
    "#パラメータの更新を許可\n",
    "for param in objModel.parameters():\n",
    "    param.requires_gred = False\n",
    "\n",
    "\n",
    "# print(model.classifier)\n",
    "\n",
    "# 出力層の最後だけ変更\n",
    "# model.classifier[5] = torch.nn.Linear(1280,2)\n",
    "objModel.classifier= nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(output_size=1),\n",
    "    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    nn.Linear(in_features=960, out_features=1280, bias=True),\n",
    "    nn.Hardswish(),\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=527, bias=True),\n",
    "    nn.BatchNorm1d(527),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=527, out_features=21, bias=True),  # 新しい層\n",
    ")\n",
    "print(objModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# エポックの数\n",
    "max_epoch = 150\n",
    "     \n",
    "# モデルの初期化\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# GPU の設定状況に基づいたデバイスの選択\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# モデルのインスタンス化とデバイスへの転送\n",
    "net3 = objModel.cuda()\n",
    "\n",
    "criterion = F.cross_entropy\n",
    "  \n",
    "# 最適化手法の選択\n",
    "optimizer = torch.optim.SGD(net3.parameters(), lr=0.1)\n",
    "# エポックごとの訓練データの損失と検証データの損失を保存するリスト\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# 学習ループ\n",
    "for epoch in range(max_epoch):\n",
    "    train_epoch_loss = 0.0\n",
    "    val_epoch_loss = 0.0\n",
    "    \n",
    "    # 訓練データでの学習\n",
    "    net3.train()\n",
    "    for batch in train_loader:\n",
    "        x, t = batch\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y = net3(x)\n",
    "        loss = criterion(y[0], t)\n",
    "        train_epoch_loss += loss.item()  # エポック全体の訓練データの損失に加算\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 訓練データでのエポックごとの平均損失を計算し保存\n",
    "    train_epoch_loss /= len(train_loader)\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    \n",
    "    # 検証データでの評価\n",
    "    net3.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, t = batch\n",
    "  \n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = net3(x)\n",
    "            loss = criterion(y[0], t)\n",
    "            val_epoch_loss += loss.item()  # エポック全体の検証データの損失に加算\n",
    "       \n",
    "    # 検証データでのエポックごとの平均損失を計算し保存\n",
    "    val_epoch_loss /= len(val_loader)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    \n",
    "    # エポックごとに損失を表示\n",
    "    print(f'Epoch [{epoch+1}/{max_epoch}], Train Loss: {train_epoch_loss:.4f}, Validation Loss: {val_epoch_loss:.4f}')\n",
    "\n",
    "\n",
    "objModel_trained = net3\n",
    "SaveModel(objModel_trained,\"objModel_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "def ShowTrainLoss(train_losses,val_losses,save):\n",
    "    # 日本時間のタイムゾーンを取得\n",
    "    jst = pytz.timezone('Asia/Tokyo')\n",
    "\n",
    "    # 現在の日本時間を取得\n",
    "    now = datetime.now(jst)\n",
    "\n",
    "    # 時刻を指定された形式の文字列に変換\n",
    "    time = \"_\"+now.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    # 学習曲線の可視化\n",
    "\n",
    "    title = 'Training and Validation Losses Object'\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Validation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "    if save == True:\n",
    "        plt.savefig(\"data/\"+title+time+\".pdf\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "ShowTrainLoss(train_losses,val_losses,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dim=1 で行ごとの最大値に対する要素番号を取得（dim=0 は列ごと）\n",
    "y_label = torch.argmax(y[0], dim=1)\n",
    "# 予測値から最大となるクラスの番号を取り出した結果\n",
    "y_label\n",
    "# 目的変数\n",
    "t\n",
    "# 値が一致しているか確認\n",
    "y_label == t\n",
    "# int => float \n",
    "torch.sum(y_label == t) * 1.0\n",
    "# 正解率\n",
    "acc = torch.sum(y_label == t) * 1.0 / len(t)\n",
    "acc\n",
    "\n",
    "\n",
    "# 正解率の計算\n",
    "def calc_acc(data_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        accs = [] # 各バッチごとの結果格納用\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            x, t = batch\n",
    "            \n",
    "            # x = torch.unsqueeze(x,1)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = net3(x)\n",
    "            \n",
    "            y_label = torch.argmax(y[0], dim=1)\n",
    "            acc = torch.sum(y_label == t) * 1.0 / len(t)\n",
    "            accs.append(acc)\n",
    "            \n",
    "    # 全体の平均を算出\n",
    "    avg_acc = torch.tensor(accs).mean()\n",
    "    print('Accuracy: {:.1f}%'.format(avg_acc * 100))\n",
    "    \n",
    "    return avg_acc\n",
    "      \n",
    "# 検証データで確認\n",
    "calc_acc(val_loader)\n",
    "# テストデータで確認\n",
    "calc_acc(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft-hard 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorDatasetの作成\n",
    "label_num = 2\n",
    "train = torch.utils.data.TensorDataset(train_feature, train_labels[:,label_num])\n",
    "val = torch.utils.data.TensorDataset(val_feature, val_labels[:,label_num])\n",
    "test = torch.utils.data.TensorDataset(test_feature, test_labels[:,label_num])\n",
    "\n",
    "# データローダに読み込む\n",
    "\n",
    "# バッチサイズ\n",
    "batch_size = 50\n",
    "\n",
    "# shuffle はデフォルトで False のため、学習データのみ True に指定\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shModel_2 = objModel_trained\n",
    "\n",
    "#パラメータの更新を許可\n",
    "for param in shModel_2.parameters():\n",
    "    param.requires_gred = False\n",
    "\n",
    "\n",
    "# print(model.classifier)\n",
    "\n",
    "# 出力層の最後だけ変更\n",
    "# model.classifier[5] = torch.nn.Linear(1280,2)\n",
    "shModel_2.classifier= nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(output_size=1),\n",
    "    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    nn.Linear(in_features=960, out_features=1280, bias=False),\n",
    "    nn.Hardswish(),\n",
    "    nn.Dropout(p=0.2, inplace=False),\n",
    "    nn.Linear(in_features=1280, out_features=527, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=527, out_features=176, bias=True),  # 新しい層\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=176, out_features=88, bias=True),  # 新しい層\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=88, out_features=21, bias=True),  # 新しい層  \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=21, out_features=2, bias=True),  # 新しい層  \n",
    ")\n",
    "print(shModel_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# エポックの数\n",
    "max_epoch = 150\n",
    "     \n",
    "# モデルの初期化\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# GPU の設定状況に基づいたデバイスの選択\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# モデルのインスタンス化とデバイスへの転送\n",
    "net4 = shModel_2.cuda()\n",
    "\n",
    "criterion = F.cross_entropy\n",
    "  \n",
    "# 最適化手法の選択\n",
    "optimizer = torch.optim.SGD(net4.parameters(), lr=0.1)\n",
    "# エポックごとの訓練データの損失と検証データの損失を保存するリスト\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# 学習ループ\n",
    "for epoch in range(max_epoch):\n",
    "    train_epoch_loss = 0.0\n",
    "    val_epoch_loss = 0.0\n",
    "    \n",
    "    # 訓練データでの学習\n",
    "    net4.train()\n",
    "    for batch in train_loader:\n",
    "        x, t = batch\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y = net4(x)\n",
    "        loss = criterion(y[0], t)\n",
    "        train_epoch_loss += loss.item()  # エポック全体の訓練データの損失に加算\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 訓練データでのエポックごとの平均損失を計算し保存\n",
    "    train_epoch_loss /= len(train_loader)\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    \n",
    "    # 検証データでの評価\n",
    "    net4.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, t = batch\n",
    "  \n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = net4(x)\n",
    "            loss = criterion(y[0], t)\n",
    "            val_epoch_loss += loss.item()  # エポック全体の検証データの損失に加算\n",
    "       \n",
    "    # 検証データでのエポックごとの平均損失を計算し保存\n",
    "    val_epoch_loss /= len(val_loader)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    \n",
    "    # エポックごとに損失を表示\n",
    "    print(f'Epoch [{epoch+1}/{max_epoch}], Train Loss: {train_epoch_loss:.4f}, Validation Loss: {val_epoch_loss:.4f}')\n",
    "\n",
    "shModel_2_trained = net4\n",
    "SaveModel(shModel_2_trained,\"shModel_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "def ShowTrainLoss(train_losses,val_losses,save):\n",
    "    # 日本時間のタイムゾーンを取得\n",
    "    jst = pytz.timezone('Asia/Tokyo')\n",
    "\n",
    "    # 現在の日本時間を取得\n",
    "    now = datetime.now(jst)\n",
    "\n",
    "    # 時刻を指定された形式の文字列に変換\n",
    "    time = \"_\"+now.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    # 学習曲線の可視化\n",
    "\n",
    "    title = 'Training and Validation Losses Soft-hard'\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Validation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "    if save == True:\n",
    "        plt.savefig(\"data/\"+title+time+\".pdf\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "ShowTrainLoss(train_losses,val_losses,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解率の計算\n",
    "def calc_acc(data_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        accs = [] # 各バッチごとの結果格納用\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            x, t = batch\n",
    "            \n",
    "            # x = torch.unsqueeze(x,1)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = net4(x)\n",
    "            \n",
    "            y_label = torch.argmax(y[0], dim=1)\n",
    "            acc = torch.sum(y_label == t) * 1.0 / len(t)\n",
    "            accs.append(acc)\n",
    "            \n",
    "    # 全体の平均を算出\n",
    "    avg_acc = torch.tensor(accs).mean()\n",
    "    print('Accuracy: {:.1f}%'.format(avg_acc * 100))\n",
    "    \n",
    "    return avg_acc\n",
    "      \n",
    "# 検証データで確認\n",
    "calc_acc(val_loader)\n",
    "# テストデータで確認\n",
    "calc_acc(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### material 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorDatasetの作成\n",
    "label_num = 1\n",
    "train = torch.utils.data.TensorDataset(train_feature, train_labels[:,label_num])\n",
    "val = torch.utils.data.TensorDataset(val_feature, val_labels[:,label_num])\n",
    "test = torch.utils.data.TensorDataset(test_feature, test_labels[:,label_num])\n",
    "\n",
    "# データローダに読み込む\n",
    "\n",
    "# バッチサイズ\n",
    "batch_size = 100\n",
    "\n",
    "# shuffle はデフォルトで False のため、学習データのみ True に指定\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matModel_2 = shModel_2_trained\n",
    "\n",
    "#パラメータの更新を許可\n",
    "for param in matModel_2.parameters():\n",
    "    param.requires_gred = False\n",
    "\n",
    "\n",
    "# print(model.classifier)\n",
    "\n",
    "# 出力層の最後だけ変更\n",
    "# model.classifier[5] = torch.nn.Linear(1280,2)\n",
    "matModel_2.classifier= nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(output_size=1),\n",
    "    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    nn.Linear(in_features=960, out_features=1280, bias=False),\n",
    "    nn.Hardswish(),\n",
    "    nn.Dropout(p=0.2, inplace=False),\n",
    "    nn.Linear(in_features=1280, out_features=527, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=527, out_features=176, bias=False),  # 新しい層\n",
    "    nn.BatchNorm1d(176),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=176, out_features=88, bias=True),  # 新しい層\n",
    "    nn.BatchNorm1d(88),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(in_features=88, out_features=7, bias=True),  # 新しい層  \n",
    ")\n",
    "print(shModel_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# エポックの数\n",
    "epoch = 0\n",
    "min_epoch = 100\n",
    "max_epoch = 150     \n",
    "# モデルの初期化\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# GPU の設定状況に基づいたデバイスの選択\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# モデルのインスタンス化とデバイスへの転送\n",
    "net5 = matModel_2.cuda()\n",
    "\n",
    "criterion = F.cross_entropy\n",
    "  \n",
    "# 最適化手法の選択\n",
    "optimizer = torch.optim.SGD(net5.parameters(), lr=0.1)\n",
    "# エポックごとの訓練データの損失と検証データの損失を保存するリスト\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# 学習ループ\n",
    "# for epoch in range(max_epoch):\n",
    "train_epoch_loss = 1.0\n",
    "val_epoch_loss = 1.0\n",
    "\n",
    "while (train_epoch_loss > 0.1 or val_epoch_loss  > 0.2) and epoch<max_epoch:\n",
    "    train_epoch_loss = 0.0\n",
    "    val_epoch_loss = 0.0\n",
    "    \n",
    "    # 訓練データでの学習\n",
    "    net5.train()\n",
    "    for batch in train_loader:\n",
    "        x, t = batch\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y = net5(x)\n",
    "        loss = criterion(y[0], t)\n",
    "        train_epoch_loss += loss.item()  # エポック全体の訓練データの損失に加算\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 訓練データでのエポックごとの平均損失を計算し保存\n",
    "    train_epoch_loss /= len(train_loader)\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    \n",
    "    # 検証データでの評価\n",
    "    net5.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, t = batch\n",
    "  \n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = net5(x)\n",
    "            loss = criterion(y[0], t)\n",
    "            val_epoch_loss += loss.item()  # エポック全体の検証データの損失に加算\n",
    "       \n",
    "    # 検証データでのエポックごとの平均損失を計算し保存\n",
    "    val_epoch_loss /= len(val_loader)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    \n",
    "    # エポックごとに損失を表示\n",
    "    print(f'Epoch [{epoch+1}/{max_epoch}], Train Loss: {train_epoch_loss:.4f}, Validation Loss: {val_epoch_loss:.4f}')\n",
    "    epoch+=1\n",
    "\n",
    "matModel_2_trained = net5\n",
    "SaveModel(matModel_2_trained,\"matModel_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matModel_2_trained = net5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "def ShowTrainLoss(train_losses,val_losses,save):\n",
    "    # 日本時間のタイムゾーンを取得\n",
    "    jst = pytz.timezone('Asia/Tokyo')\n",
    "\n",
    "    # 現在の日本時間を取得\n",
    "    now = datetime.now(jst)\n",
    "\n",
    "    # 時刻を指定された形式の文字列に変換\n",
    "    time = \"_\"+now.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    # 学習曲線の可視化\n",
    "\n",
    "    title = 'Training and Validation Losses Material'\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Validation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "    if save == True:\n",
    "        plt.savefig(\"data/\"+title+time+\".pdf\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "ShowTrainLoss(train_losses,val_losses,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dim=1 で行ごとの最大値に対する要素番号を取得（dim=0 は列ごと）\n",
    "y_label = torch.argmax(y[0], dim=1)\n",
    "# 予測値から最大となるクラスの番号を取り出した結果\n",
    "y_label\n",
    "# 目的変数\n",
    "t\n",
    "# 値が一致しているか確認\n",
    "y_label == t\n",
    "# int => float \n",
    "torch.sum(y_label == t) * 1.0\n",
    "# 正解率\n",
    "acc = torch.sum(y_label == t) * 1.0 / len(t)\n",
    "acc\n",
    "\n",
    "\n",
    "# 正解率の計算\n",
    "def calc_acc(data_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        accs = [] # 各バッチごとの結果格納用\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            x, t = batch\n",
    "            \n",
    "            # x = torch.unsqueeze(x,1)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = net5(x)\n",
    "            \n",
    "            y_label = torch.argmax(y[0], dim=1)\n",
    "            acc = torch.sum(y_label == t) * 1.0 / len(t)\n",
    "            accs.append(acc)\n",
    "            \n",
    "    # 全体の平均を算出\n",
    "    avg_acc = torch.tensor(accs).mean()\n",
    "    print('Accuracy: {:.1f}%'.format(avg_acc * 100))\n",
    "    \n",
    "    return avg_acc\n",
    "      \n",
    "# 検証データで確認\n",
    "calc_acc(val_loader)\n",
    "# テストデータで確認\n",
    "calc_acc(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### object 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorDatasetの作成\n",
    "label_num = 0\n",
    "train = torch.utils.data.TensorDataset(train_feature, train_labels[:,label_num])\n",
    "val = torch.utils.data.TensorDataset(val_feature, val_labels[:,label_num])\n",
    "test = torch.utils.data.TensorDataset(test_feature, test_labels[:,label_num])\n",
    "\n",
    "# データローダに読み込む\n",
    "\n",
    "# バッチサイズ\n",
    "batch_size = 100\n",
    "\n",
    "# shuffle はデフォルトで False のため、学習データのみ True に指定\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objModel_2 = matModel_2_trained\n",
    "\n",
    "# パラメータの更新を許可\n",
    "for param in objModel_2.parameters():\n",
    "    param.requires_grad =  True # requires_gred -> requires_grad\n",
    "\n",
    "# 新しい層の追加とBatch Normalizationの導入\n",
    "objModel_2.classifier = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(output_size=1),\n",
    "    nn.Flatten(start_dim=1, end_dim=-1),\n",
    "    nn.Linear(in_features=960, out_features=1280, bias=True),\n",
    "    nn.Hardswish(),\n",
    "    nn.Dropout(p=0.5, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=527, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=527, out_features=176, bias=True),  # 新しい層\n",
    "    nn.BatchNorm1d(176),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=176, out_features=88, bias=True),  # 新しい層\n",
    "    nn.BatchNorm1d(88),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=88, out_features=21, bias=True),  # 新しい層  \n",
    ")\n",
    "\n",
    "print(objModel_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# エポックの数\n",
    "epoch = 0\n",
    "max_epoch = 1000     \n",
    "# モデルの初期化\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# GPU の設定状況に基づいたデバイスの選択\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# モデルのインスタンス化とデバイスへの転送\n",
    "net6 = objModel_2.cuda()\n",
    "\n",
    "criterion = F.cross_entropy\n",
    "  \n",
    "# 最適化手法の選択\n",
    "optimizer = torch.optim.SGD(net6.parameters(), lr=0.1)\n",
    "# エポックごとの訓練データの損失と検証データの損失を保存するリスト\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_epoch_loss = 1.0\n",
    "val_epoch_loss = 1.0\n",
    "# 学習ループ\n",
    "while (train_epoch_loss > 0.1 or val_epoch_loss  > 0.6) and epoch<max_epoch:\n",
    "    train_epoch_loss = 0.0\n",
    "    val_epoch_loss = 0.0\n",
    "    \n",
    "    # 訓練データでの学習\n",
    "    net6.train()\n",
    "    for batch in train_loader:\n",
    "        x, t = batch\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y = net6(x)\n",
    "        loss = criterion(y[0], t)\n",
    "        train_epoch_loss += loss.item()  # エポック全体の訓練データの損失に加算\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 訓練データでのエポックごとの平均損失を計算し保存\n",
    "    train_epoch_loss /= len(train_loader)\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    \n",
    "    # 検証データでの評価\n",
    "    net6.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, t = batch\n",
    "  \n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = net6(x)\n",
    "            loss = criterion(y[0], t)\n",
    "            val_epoch_loss += loss.item()  # エポック全体の検証データの損失に加算\n",
    "       \n",
    "    # 検証データでのエポックごとの平均損失を計算し保存\n",
    "    val_epoch_loss /= len(val_loader)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    \n",
    "    # エポックごとに損失を表示\n",
    "    print(f'Epoch [{epoch+1}/{max_epoch}], Train Loss: {train_epoch_loss:.4f}, Validation Loss: {val_epoch_loss:.4f}')\n",
    "    epoch+=1\n",
    "\n",
    "objModel_2_trained = net6\n",
    "SaveModel(objModel_2_trained,\"objModel_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objModel_2_trained = net6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "def ShowTrainLoss(train_losses,val_losses,save):\n",
    "    # 日本時間のタイムゾーンを取得\n",
    "    jst = pytz.timezone('Asia/Tokyo')\n",
    "\n",
    "    # 現在の日本時間を取得\n",
    "    now = datetime.now(jst)\n",
    "\n",
    "    # 時刻を指定された形式の文字列に変換\n",
    "    time = \"_\"+now.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    # 学習曲線の可視化\n",
    "\n",
    "    title = 'Training and Validation Losses Material'\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Validation')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "    if save == True:\n",
    "        plt.savefig(\"data/\"+title+time+\".pdf\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "ShowTrainLoss(train_losses,val_losses,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解率の計算\n",
    "def calc_acc(data_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        accs = [] # 各バッチごとの結果格納用\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            x, t = batch\n",
    "            \n",
    "            # x = torch.unsqueeze(x,1)\n",
    "            \n",
    "            x = x.to(device)\n",
    "            t = t.to(device)\n",
    "            y = net6(x)\n",
    "            \n",
    "            y_label = torch.argmax(y[0], dim=1)\n",
    "            acc = torch.sum(y_label == t) * 1.0 / len(t)\n",
    "            accs.append(acc)\n",
    "            \n",
    "    # 全体の平均を算出\n",
    "    avg_acc = torch.tensor(accs).mean()\n",
    "    print('Accuracy: {:.1f}%'.format(avg_acc * 100))\n",
    "    \n",
    "    return avg_acc\n",
    "      \n",
    "# 検証データで確認\n",
    "calc_acc(val_loader)\n",
    "# テストデータで確認\n",
    "calc_acc(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
