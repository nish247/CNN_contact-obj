{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 50 66150 1050\n"
     ]
    }
   ],
   "source": [
    "# 学習データセットの読み込み\n",
    "# import pickle\n",
    "# f = open('soundDataset_20240619.pickle','rb')\n",
    "# soundDataset_new = pickle.load(f)\n",
    "\n",
    "# patterns = soundDataset_new.shape[0] #物体の種類\n",
    "# trials = soundDataset_new.shape[1] #試行数\n",
    "# len_data = soundDataset_new.shape[2] #1試行の長さ,サンプル数\n",
    "# num_data = patterns*trials #全試行数\n",
    "\n",
    "# print(patterns,trials,len_data,num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9097600,)\n",
      "(66150,)\n",
      "(128, 130)\n",
      "torch.Size([1, 1, 128, 130])\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "# path_audio_file = \"./data/origin_data/origindata_20240619/01_Y-shirt/01_Y-shirt.WAV\"\n",
    "\n",
    "sr = 44100\n",
    "test_data = librosa.load(path_audio_file,sr=sr)\n",
    "print(test_data[0].shape)\n",
    "\n",
    "#データのトリミング\n",
    "origin_data = test_data[0]\n",
    "flag_amp = 0.1 #各試行の合図を検知する基準振幅\n",
    "trimSkip = int(sr*0.4)\n",
    "trimTime = int(sr*1.5)#1試行あたりのデータ長\n",
    "index = 0\n",
    "while origin_data[index] >= flag_amp:\n",
    "    trimData = np.array(origin_data[index+trimSkip:index+trimTime+trimSkip])\n",
    "    index+=1\n",
    "print(trimData.shape)\n",
    "\n",
    "#データの保存\n",
    "output_path = \"./demo/trim_datas/sample.wav\"\n",
    "sf.write(output_path,trimData,sr)\n",
    "\n",
    "trimData = soundDataset_new[11][10]\n",
    "\n",
    "#メルスペクトログラムに変換\n",
    "spectrogram = librosa.feature.melspectrogram(y=trimData, sr=sr)# スペクトログラムを計算\n",
    "spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "print(spectrogram_db.shape)\n",
    "\n",
    "#tensorに変換\n",
    "melspec_tensor = torch.tensor(spectrogram_db, dtype=torch.float32)\n",
    "# test_loader = torch.utils.data.DataLoader(melspec_tensor)\n",
    "test_loader = melspec_tensor.unsqueeze(0).unsqueeze(0)\n",
    "print(test_loader.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor([11], device='cuda:0')\n",
      "tensor(16.2104, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#推定モデルの読み込み\n",
    "objModel_path = \"./demo/models/objModel.pth\"\n",
    "load_objModel = torch.load(objModel_path)\n",
    "objModel = load_objModel.eval()\n",
    "\n",
    "# 検証データでの評価\n",
    "with torch.no_grad():\n",
    "        x = test_loader.to(device)\n",
    "        # print(x)\n",
    "        y = objModel(x)\n",
    "        pred = torch.argmax(y[0], dim=1)\n",
    "        print(pred)\n",
    "        confidence = torch.max(y[0])\n",
    "        print(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
